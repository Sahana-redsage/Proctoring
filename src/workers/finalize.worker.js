const { Worker } = require("bullmq");
const path = require("path");
const fs = require("fs");
const { exec } = require("child_process");
const pool = require("../config/db");
const { uploadToR2 } = require("../services/r2.service");
const { finalizeQueue, batchQueue, redisConnection } = require("../config/bullmq");

require("dotenv").config();

console.log("üî¥ Finalize Worker started");

// Utility: run shell commands safely
function run(cmd) {
  return new Promise((resolve, reject) => {
    exec(cmd, (err, stdout, stderr) => {
      if (err) return reject(stderr || err.message);
      resolve(stdout);
    });
  });
}

const worker = new Worker(
  "finalizeQueue",
  async job => {
    const { sessionId } = job.data;

    console.log(`üé¨ Finalizing session ${sessionId}`);
    const client = await pool.connect();

    try {
      /**
       * 1Ô∏è‚É£ CHECK ALL CHUNKS STATUS
       */
      const { rows: chunks } = await client.query(
        `
        SELECT id, chunk_index, status, file_path
        FROM proctoring_chunks
        WHERE session_id = $1
        ORDER BY chunk_index
        `,
        [sessionId]
      );

      if (!chunks.length) {
        console.log(`‚ö†Ô∏è [${sessionId}] No chunks found`);
        return;
      }

      const incomplete = chunks.some(
        c => c.status !== "PROCESSED"
      );

      if (incomplete) {
        console.log(
          `‚è∏Ô∏è [${sessionId}] Some chunks still processing. Retrying finalize...`
        );

        // üîÅ Requeue finalize with delay
        await finalizeQueue.add(
          "FINALIZE_SESSION",
          { sessionId },
          { delay: 5000, attempts: 10 }
        );

        return;
      }

      /**
       * 1.5Ô∏è‚É£ TRIGGER FINAL BATCH (LEFTOVERS)
       */
      const BATCH_SIZE = 3; // Keep consistent with controller
      const totalChunks = chunks.length;

      // Calculate the starting index for any remaining chunks that didn't form a full batch
      const remainderStart = Math.floor(totalChunks / BATCH_SIZE) * BATCH_SIZE;

      if (remainderStart < totalChunks) {
        console.log(`üß© [${sessionId}] Triggering final batch for leftovers: ${remainderStart} ‚Üí ${totalChunks - 1}`);
        await batchQueue.add("PROCESS_BATCH", {
          sessionId,
          fromChunkIndex: remainderStart,
          toChunkIndex: totalChunks - 1,
          isFinalBatch: true
        });
      } else if (totalChunks > 0) {
        // Aligned perfectly, but we need to run Final Batch to flush events
        console.log(`üß© [${sessionId}] Triggering final flush (re-run last batch): ${remainderStart - BATCH_SIZE} ‚Üí ${remainderStart - 1}`);
        await batchQueue.add("PROCESS_BATCH", {
          sessionId,
          fromChunkIndex: remainderStart - BATCH_SIZE,
          toChunkIndex: remainderStart - 1,
          isFinalBatch: true
        });
      }

      /**
       * 2Ô∏è‚É£ PREPARE CONCAT FILE FOR FFMPEG
       */
      const mergeDir = path.join(
        process.cwd(),
        "tmp",
        "merge"
      );

      if (!fs.existsSync(mergeDir)) {
        fs.mkdirSync(mergeDir, { recursive: true });
      }

      const concatFile = path.join(
        mergeDir,
        `${sessionId}_concat.txt`
      );

      const concatContent = chunks
        .map(c => `file '${c.file_path.replace(/\\/g, "/")}'`)
        .join("\n");

      fs.writeFileSync(concatFile, concatContent);

      /**
       * 3Ô∏è‚É£ MERGE VIDEO USING FFMPEG (NO RE-ENCODE)
       */
      const outputFile = path.join(
        mergeDir,
        `${sessionId}_final.webm`
      );

      console.log(
        `‚è≥ [${sessionId}] Starting video merge of ${chunks.length} chunks...`
      );

      await run(
        `ffmpeg -y -f concat -safe 0 -i "${concatFile}" -c copy "${outputFile}"`
      );

      console.log(
        `üéûÔ∏è [${sessionId}] Video merged successfully at ${outputFile}`
      );

      /**
       * 4Ô∏è‚É£ UPLOAD TO CLOUDFLARE R2
       */
      const r2Key = `proctoring/${sessionId}.webm`;

      console.log(
        `üì§ [${sessionId}] Uploading to R2 (${r2Key})...`
      );

      const finalVideoUrl = await uploadToR2(
        outputFile,
        r2Key
      );

      console.log(
        `‚òÅÔ∏è [${sessionId}] Uploaded to R2: ${finalVideoUrl}`
      );

      /**
       * 5Ô∏è‚É£ UPDATE SESSION RECORD
       */
      await client.query(
        `
        UPDATE proctoring_sessions
        SET
          final_video_url = $1,
          status = 'DONE',
          ended_at = NOW()
        WHERE id = $2
        `,
        [finalVideoUrl, sessionId]
      );

      console.log(
        `üíæ [${sessionId}] Database updated with final video URL`
      );

      /**
       * 6Ô∏è‚É£ CLEANUP TEMP FILES
       */
      console.log(
        `üßπ [${sessionId}] Cleaning up local files...`
      );

      /*
      for (const c of chunks) {
        if (fs.existsSync(c.file_path)) {
          fs.unlinkSync(c.file_path);
        }
      }

      if (fs.existsSync(concatFile)) fs.unlinkSync(concatFile);
      // if (fs.existsSync(outputFile)) fs.unlinkSync(outputFile); // Keep output too? Well output is uploaded.
      */

      console.log(
        `‚úÖ [${sessionId}] Session finalized successfully`
      );
    } catch (err) {
      console.error(
        `‚ùå [${sessionId}] Finalize failed:`,
        err
      );
      throw err;
    } finally {
      client.release();
    }
  },
  {
    connection: redisConnection
  }
);

module.exports = worker;
